{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "petal_snorkel_challenge.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8PgkE1uO-nH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxj4vH3lO_sJ"
      },
      "source": [
        "# Data Processing Challenge\n",
        "In machine learning classification, it is important to consider the size of your dataset and how many labels you want to predict. In this example we demonstrate how to use snorkel to read through a series of papers and mark which rules match. So for example, the sentence \"Birds fly with wings.\" We want to use rules that take the sentence and search for \"fly\", if the word exists then we apply the label \"flight\". **The paper containing this word has to do with flight.**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNGLbm4lQIpl"
      },
      "source": [
        "## Data walk through\n",
        "\n",
        "In the files section there are two csv files: *biomimicry_function_rules.csv* and *biomimicry_functions_enumerated.csv*. \n",
        "\n",
        "biomimicry_functions_enumerated.csv\n",
        "- First column contains the labels\n",
        "- Second column contains the label id\n",
        "\n",
        "biomimicry_function_rules.csv \n",
        "- Header: these are the labels from biomimicry_functions_enumerated.csv column 1\n",
        "- Each row below the header contains words. If any word matches the text then we mark it with a label.\n",
        "\n",
        "There's another folder called `david_work` inside there's 2 csv files *formatted_enums.csv* and *labeled_data.csv*. labeled_data.csv is the important one. This contains the paper title and abstract which are combined as a text for prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziwkbwwaWStC"
      },
      "source": [
        "The code below loads the dataset and displays the columns. Not all the columns are used for prediction. Only the column called 'text' is used. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "SU9wsEflRN_b",
        "outputId": "7c837458-4751-4742-b218-6ef322f99855"
      },
      "source": [
        "from utils import load_dataset\n",
        "df_train, df_test = load_dataset() \n",
        "print(df_train.columns) "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-43f246a2dc1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(load_train_labels, split_dev_valid)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_train_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_dev_valid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"david_work/labeled_data.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m#lowercase column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'david_work/labeled_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx8BVgKvWdFl"
      },
      "source": [
        "# Challenge setup\n",
        "This part of the code illustrates how to setup the environment for this challenge. This example uses snorkel. Snorkel takes a list of texts and applies rules that predict labels. S\n",
        "\n",
        "Some texts may match rules from many labels like \"attach_permanently\" and \"send_sound_signals\". This could be texts about bats. Snorkel uses supervised machine learning to take the set of matching rules and classify papers with a particular label.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpLKX0K3ae8p"
      },
      "source": [
        "## Cloning the data\n",
        "Run the code below to clone the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIrFTQYgZsHN"
      },
      "source": [
        "!git clone https://github.com/nasa-petal/interview_questions.git\n",
        "!mv snorkel_challenge/* .\n",
        "!rm snorkel_challenge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8hU_6sCakwE"
      },
      "source": [
        "The code below installs the latest dependancies for snorkel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JweFvhQMan6G"
      },
      "source": [
        "!pip install -U networkx munkres numpy scipy pandas scikit-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfRnxdmPbJ2D"
      },
      "source": [
        "import pandas as pd \n",
        "from snorkel.labeling import LabelingFunction\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzVCvhsoborF"
      },
      "source": [
        "def keyword_lookup(x,bio_functions:pd.DataFrame,bio_function_rules:pd.DataFrame):\n",
        "    \"\"\"Returns the id corresponding to the label\n",
        "\n",
        "    Args:\n",
        "        x (str): some phrase\n",
        "\n",
        "    Returns:\n",
        "        int: the id\n",
        "    \"\"\"\n",
        "    for i in range(len(bio_functions)):\n",
        "        label_name = bio_functions.iloc[i]['function'] \n",
        "        label_id = bio_functions.iloc[i]['function_enumerated']        \n",
        "        \n",
        "        label_rule_name = label_name + \"_rules\"\n",
        "        if label_rule_name in list(bio_function_rules.columns):\n",
        "            phrases_to_look_for = bio_function_rules[label_rule_name].to_list()\n",
        "            phrases_to_look_for = [x for x in phrases_to_look_for if x == 'nan']\n",
        "            for phrase in phrases_to_look_for:\n",
        "                # now you could make a counter and see the percentage match so if 10/20 phrases are in the text/abstract then you return the\n",
        "                if phrase in x.text.lower():     \n",
        "                    return label_id \n",
        "    return -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcPSgoT8a-o1"
      },
      "source": [
        "def create_labeling_functions(bio_file:pd.DataFrame, bio_rules:pd.DataFrame):\n",
        "    \"\"\"create a list of labeling functions\n",
        "\n",
        "    Args:\n",
        "        bio_file (pd.DataFrame): a list of all the biomimicry functions\n",
        "        bio_rules (pd.DataFrame): a list of all the 'rules' for each biomimicry function\n",
        "\n",
        "    Returns:\n",
        "        labeling_function_list: a list of all the labeling function 'rules' corresponding to each biomimicry function\n",
        "    \"\"\"\n",
        "    bio_file = pd.read_csv(bio_file)\n",
        "    bio_rules = pd.read_csv(bio_rules)\n",
        "\n",
        "    lst = []\n",
        "    underscore_list = []\n",
        "    rules_no_na = []\n",
        "    labeling_function_list = []\n",
        "    \n",
        "    #get a list of all the rules\n",
        "    for i in range(len(bio_file)):\n",
        "        label_name = bio_file.iloc[i]['function'] \n",
        "        label_rule_name = label_name + \"_rules\"\n",
        "        if label_rule_name in list(bio_rules.columns):\n",
        "            phrases_lst = bio_rules[label_rule_name].to_list()\n",
        "            lst.append(phrases_lst)\n",
        "    chained_lst = (list(itertools.chain.from_iterable(lst)))\n",
        "    #remove blank cells\n",
        "    remove_na = [x for x in chained_lst if pd.isnull(x) == False]\n",
        "    #remove duplicates\n",
        "    for rule in remove_na:\n",
        "        if rule not in rules_no_na:\n",
        "            rules_no_na.append(rule)\n",
        "    #add underscore to rules\n",
        "    for item in rules_no_na:\n",
        "        item = item.replace(\" \", \"_\")\n",
        "        underscore_list.append(item)\n",
        "    #create labeling function for each rule\n",
        "    for phrase in underscore_list:\n",
        "        labeling_function = LabelingFunction(name=f\"keyword_{phrase}\", f=keyword_lookup,\n",
        "                        resources={\"bio_functions\":bio_file,\"bio_function_rules\":bio_rules})\n",
        "        labeling_function_list.append(labeling_function)\n",
        "\n",
        "    # print(len(labeling_function_list))\n",
        "    return labeling_function_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9wjHBTUbqdj"
      },
      "source": [
        "labeling_function_list = create_labeling_functions(r'./biomimicry_functions_enumerated.csv', r'./biomimicry_function_rules.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbzLdviCb4qs"
      },
      "source": [
        "## Training Problem\n",
        "The code below shows how to train using snorkel. Note the training probably won't work because it consumes an enormous about of memory.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_yq5eESb_sp"
      },
      "source": [
        "len(labeling_function_list)\n",
        "\n",
        "if not os.path.exists('lf_analysis.pickle'):\n",
        "    applier = PandasLFApplier(lfs=labeling_function_list)\n",
        "    # define train and test sets\n",
        "    L_train = applier.apply(df=df_train)\n",
        "    L_test = applier.apply(df=df_test)\n",
        "\n",
        "    df = LFAnalysis(L=L_train, lfs=labeling_function_list).lf_summary()\n",
        "    with open('lf_analysis.pickle','wb') as f:\n",
        "        pickle.dump({\"lf_analysis\":df, 'L_train':L_train,'L_test':L_test},f)\n",
        "\n",
        "if os.path.exists('lf_analysis.pickle'):\n",
        "    with open('lf_analysis.pickle','rb') as f:\n",
        "        data = pickle.load(f)\n",
        "        lf_analysis = data['lf_analysis']\n",
        "        L_train = data['L_train']\n",
        "        L_test = data['L_test']\n",
        "\n",
        "majority_model = MajorityLabelVoter()\n",
        "preds_train = majority_model.predict(L=L_train)\n",
        "\n",
        "label_model = LabelModel(cardinality=19, verbose=True, device='cpu')\n",
        "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=123)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Votdntc9cbNw"
      },
      "source": [
        "### Why Training doesn't work\n",
        "Looking at L_train variable you can see that the labels go up to 97 but there's a few skips. The skipped labels are not being used but snorkel thinks that there's a total of 97 labels so it allocates the memory.  The other problem is the total number of rules. Because there's so many rules you a matrix of size #papers(trainset) x #rules where the total number of rules is 660"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxVGkOtpdM3m"
      },
      "source": [
        "# add code to show this"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYFamHt0Zo9u"
      },
      "source": [
        "# The Challenge\n",
        "The number of labels as well as the number of rules influences the size of the neural network used by snorkel. You need to find a way to programmatically reduce number of rules and labels. "
      ]
    }
  ]
}