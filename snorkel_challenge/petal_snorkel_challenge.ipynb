{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "petal_snorkel_challenge.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxj4vH3lO_sJ"
      },
      "source": [
        "# Data Processing Challenge\n",
        "In machine learning classification, it is important to consider the size of your dataset and how many labels you want to predict. In this example we demonstrate how to use snorkel to read through a series of papers and mark which rules match. So for example, the sentence \"Birds fly with wings.\" We want to use rules that take the sentence and search for \"fly\", if the word exists then we apply the label \"flight\". \n",
        "\n",
        "# Downloading the Dataset\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeMcdFEwYXHy",
        "outputId": "0d39f8be-dd9b-4a09-9160-4ad180c00df9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# run this to reset the folders \n",
        "!rm -r david_work/ interview_questions/ petal_snorkel/ sample_data/ bio* utils.py snorkel_challenge"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'sample_data/': No such file or directory\n",
            "rm: cannot remove 'snorkel_challenge': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZc_mCMFQt7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2199c7ca-08ff-4810-9a24-9c7ccd901f2a"
      },
      "source": [
        "!git clone https://github.com/nasa-petal/interview_questions.git"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'interview_questions'...\n",
            "remote: Enumerating objects: 253, done.\u001b[K\n",
            "remote: Counting objects: 100% (253/253), done.\u001b[K\n",
            "remote: Compressing objects: 100% (237/237), done.\u001b[K\n",
            "remote: Total 253 (delta 16), reused 241 (delta 11), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (253/253), 1.55 MiB | 9.98 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFvFFtyVTDuW"
      },
      "source": [
        "# re-arrange some files \n",
        "!mv interview_questions/snorkel_challenge/* ."
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7VlpiK0YvhP"
      },
      "source": [
        "!rm -r interview_questions"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xwYdZXBZDBB"
      },
      "source": [
        "!mv snorkel/ paht\n",
        "!mv paht/snorkel .\n",
        "!rm -r paht  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNGLbm4lQIpl"
      },
      "source": [
        "## Data walk through\n",
        "\n",
        "In the files section there are two csv files: *biomimicry_function_rules.csv* and *biomimicry_functions_enumerated.csv*. \n",
        "\n",
        "biomimicry_functions_enumerated.csv\n",
        "- First column contains the labels\n",
        "- Second column contains the label id\n",
        "\n",
        "biomimicry_function_rules.csv \n",
        "- Header: these are the labels from biomimicry_functions_enumerated.csv column 1\n",
        "- Each row below the header contains words. If any word matches the text then we mark it with a label.\n",
        "\n",
        "There's another folder called `david_work` inside there's 2 csv files *formatted_enums.csv* and *labeled_data.csv*. labeled_data.csv is the important one. This contains the paper title and abstract which are combined as a text for prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziwkbwwaWStC"
      },
      "source": [
        "The code below loads the dataset and displays the columns. Not all the columns are used for prediction. Only the column called 'text' is used. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU9wsEflRN_b",
        "outputId": "bc735b22-34fa-40fc-cc65-f95ffcc11554"
      },
      "source": [
        "from petal_utils import load_dataset\n",
        "df_train, df_test = load_dataset()\n",
        "print(df_train.columns) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['doi', 'url', 'full_doc_link', 'is_open_access', 'label_level_1',\n",
            "       'label_level_2', 'label_level_3', 'journal', 'literature_site',\n",
            "       'unnamed: 11', 'label', 'text'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx8BVgKvWdFl"
      },
      "source": [
        "# Challenge setup\n",
        "This part of the code illustrates how to setup the environment for this challenge. This example uses snorkel. Snorkel takes a list of texts and applies rules that predict labels. S\n",
        "\n",
        "Some texts may match rules from many labels like \"attach_permanently\" and \"send_sound_signals\". This could be texts about bats. Snorkel uses supervised machine learning to take the set of matching rules and classify papers with a particular label.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpLKX0K3ae8p"
      },
      "source": [
        "## Installing prerequisites\n",
        "Run the code below to install dependancies for snorkel. You may need to restart your runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JweFvhQMan6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7812b9d1-6c11-4d82-fca7-3f590a386e5b"
      },
      "source": [
        "!pip install -U networkx munkres numpy scipy pandas scikit-learn"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (2.6.3)\n",
            "Requirement already satisfied: munkres in /usr/local/lib/python3.7/dist-packages (1.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfRnxdmPbJ2D"
      },
      "source": [
        "import pandas as pd \n",
        "import itertools, os, pickle\n",
        "from snorkel.labeling import LabelingFunction\n",
        "from snorkel.labeling.model import LabelModel\n",
        "from snorkel.labeling import PandasLFApplier\n",
        "from snorkel.labeling import LFAnalysis\n",
        "from snorkel.labeling.model import MajorityLabelVoter\n",
        "from petal_utils import load_dataset"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzVCvhsoborF"
      },
      "source": [
        "def keyword_lookup(x,phrase_to_match:str, label_id:int):\n",
        "    \"\"\"Snorkel Labeling function. Purpose is to return the label id based on the phrase in sentence\n",
        "\n",
        "    Args:\n",
        "        phrase_to_match (str): some phrase that we need to match\n",
        "        label_id (int): id of label to use for this match\n",
        "    Returns:\n",
        "        (int): label id if match or -1 if no match \n",
        "    \"\"\"\n",
        "    if phrase_to_match.lower() in x.text.lower():     \n",
        "        return label_id\n",
        "    else:\n",
        "        return -1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcPSgoT8a-o1"
      },
      "source": [
        "def create_labeling_functions(bio_file:pd.DataFrame, bio_rules:pd.DataFrame):\n",
        "    \"\"\"create a list of labeling functions\n",
        "    \n",
        "    Args:\n",
        "        bio_file (pd.DataFrame): a list of all the biomimicry functions\n",
        "        bio_rules (pd.DataFrame): a list of all the 'rules' for each biomimicry function\n",
        "    Returns:\n",
        "        labeling_function_list: a list of all the labeling function 'rules' corresponding to each biomimicry function\n",
        "    \"\"\"\n",
        "    bio_file = pd.read_csv(bio_file)\n",
        "    bio_rules = pd.read_csv(bio_rules)\n",
        "\n",
        "    names_used = list()\n",
        "    labeling_function_list = list()\n",
        "    \n",
        "    #get a list of all the rules\n",
        "    for i in range(len(bio_file)):\n",
        "\n",
        "        label_name = bio_file.iloc[i]['function']\n",
        "        label_id = bio_file.iloc[i]['function_enumerated']\n",
        "        label_rule_name = label_name + \"_rules\"\n",
        "\n",
        "        if label_rule_name in list(bio_rules.columns):\n",
        "            underscore_list = []\n",
        "            phrases_lst = bio_rules[label_rule_name].to_list()\n",
        "            \n",
        "            #remove blank cells and keep unique values \n",
        "            rules_no_na = list(set([x for x in phrases_lst if not pd.isnull(x)]))\n",
        "            \n",
        "            #add underscore to rules\n",
        "            for item in rules_no_na:\n",
        "                item = item.replace(\" \", \"_\")\n",
        "                underscore_list.append(item)\n",
        "            #create labeling function for each rule\n",
        "            for phrase in underscore_list:\n",
        "                function_name = f\"keyword_{label_id}_{phrase}\"\n",
        "                if (function_name not in names_used):\n",
        "                    labeling_function = LabelingFunction(name=function_name, f=keyword_lookup,\n",
        "                                    resources={\"phrase_to_match\":phrase, \"label_id\":label_id})\n",
        "                    labeling_function_list.append(labeling_function)\n",
        "                    names_used.append(function_name)\n",
        "    \n",
        "    return labeling_function_list"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwiSJXHqoARU"
      },
      "source": [
        "There is a total of 674 rules. **Remember this for later on**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9wjHBTUbqdj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "601e8af8-a46e-424a-c847-e234f9d11b1f"
      },
      "source": [
        "labeling_function_list = create_labeling_functions(r'./biomimicry_functions_enumerated.csv', r'./biomimicry_function_rules.csv')\n",
        "len(labeling_function_list)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "674"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbzLdviCb4qs"
      },
      "source": [
        "## Training Problem\n",
        "The code below shows how to train using snorkel. Note the training probably won't work because it consumes an enormous about of memory.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_yq5eESb_sp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "351d7519-7840-485a-9abd-f65580cf4376"
      },
      "source": [
        "df_train, df_test = load_dataset()\n",
        "\n",
        "labeling_function_list = create_labeling_functions(r'./biomimicry_functions_enumerated.csv', r'./biomimicry_function_rules.csv')\n",
        "\n",
        "len(labeling_function_list)\n",
        "\n",
        "applier = PandasLFApplier(lfs=labeling_function_list)\n",
        "# define train and test sets\n",
        "L_train = applier.apply(df=df_train)\n",
        "L_test = applier.apply(df=df_test)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 107/107 [00:01<00:00, 81.09it/s]\n",
            "100%|██████████| 14/14 [00:00<00:00, 85.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su9TURMHnVOZ"
      },
      "source": [
        "## Crashing the runtime\n",
        "Running the following block of code consumes way too much memory and causes the runtime to crash. You will have lots of fun if you run the code below. Skip this section and go to the one below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmwmoHtCmTY3"
      },
      "source": [
        "majority_model = MajorityLabelVoter(cardinality=98)\n",
        "preds_train = majority_model.predict(L=L_train)\n",
        "\n",
        "label_model = LabelModel(cardinality=98, verbose=True, device = 'cpu')\n",
        "label_model.fit(L_train=L_train, n_epochs=1000, log_freq=100, seed=123)\n",
        "\n",
        "LFAnalysis(L=L_train, lfs=labeling_function_list).lf_summary()\n",
        "\n",
        "df_train_filtered, preds_train_filtered = filter_unlabeled_dataframe(\n",
        "    X=df_train, y=preds_train, L=L_train)\n",
        "\n",
        "df_train[\"label\"] = label_model.predict(L=L_train, tie_break_policy=\"abstain\")\n",
        "\n",
        "label_model.save(\"snorkel_model.pkl\")\n",
        "\n",
        "df_train.to_csv(\"results.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Votdntc9cbNw"
      },
      "source": [
        "### Why Training doesn't work\n",
        "Looking at L_train variable you can see that the labels go up to 97 but there's a a lot skips. The skipped labels are not being used but snorkel thinks that there's a total of 97 labels so it allocates an enormous amount of memory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxVGkOtpdM3m",
        "outputId": "b1dc9f15-435d-49e8-89fd-1deb53b9a9c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "np.unique(L_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1,  0,  1,  6,  8, 19, 20, 21, 24, 26, 30, 35, 39, 40, 96, 97])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WTwNDx5n3v3"
      },
      "source": [
        "The other problem is the total number of rules. Because there's so many rules you a matrix of size #papers(trainset) x #rules where the total number of rules is 674. The other number 107 is the number of texts to train. 14 is number of texts for train. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv8jyPyPn5Ko",
        "outputId": "f5fb9af5-a0d8-4d92-91f0-60284f5bab70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(L_train.shape)\n",
        "print(L_test.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(107, 674)\n",
            "(14, 674)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYFamHt0Zo9u"
      },
      "source": [
        "# The Challenge\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi0908odo9Sp"
      },
      "source": [
        "## First Challenge - Reduction of the dataset\n",
        "The number of labels as well as the number of rules influences the size of the neural network used by snorkel. You need to find a way to programmatically reduce number of rules and labels. Instead of 97 we should see 16 labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVcqd1mDomR6",
        "outputId": "98325000-f0b5-42ee-bcac-aaef97d3d1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(np.unique(L_train))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpZ-Kb8Co4Av"
      },
      "source": [
        "## Second Challenge - further reduction and overlap of labels in each model\n",
        "Overlapping is important for verifying if the label is correctly matched.\n",
        "\n",
        "For example:\n",
        "\n",
        "*   Model 1 - Predicts: Labels 1-4 + (Not labels 1-4)\n",
        "*   Model 2 - Predicts: labels 3-7 + (Not labels 3-7)\n",
        "\n",
        "If both Model 1 and Model 2 predict that this text matches label 4 then most likely it is label 4.\n",
        "\n",
        "Lets split the dataset into groups that can be used to train something like this."
      ]
    }
  ]
}