{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "petal_snorkel_challenge.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxj4vH3lO_sJ"
      },
      "source": [
        "# Data Processing Challenge\n",
        "In machine learning classification, it is important to consider the size of your dataset and how many labels you want to predict. In this example we demonstrate how to use snorkel to read through a series of papers and mark which rules match. So for example, the sentence \"Birds fly with wings.\" We want to use rules that take the sentence and search for \"fly\", if the word exists then we apply the label \"flight\". \n",
        "\n",
        "# Downloading the Dataset\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZc_mCMFQt7I",
        "outputId": "034d7e55-aefb-4992-f1a1-ec9abb26037f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/nasa-petal/interview_questions.git\n",
        "mv interview_questions/snorkel_challenge/* ."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'interview_questions'...\n",
            "remote: Enumerating objects: 245, done.\u001b[K\n",
            "remote: Counting objects: 100% (245/245), done.\u001b[K\n",
            "remote: Compressing objects: 100% (232/232), done.\u001b[K\n",
            "remote: Total 245 (delta 10), reused 234 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (245/245), 1.54 MiB | 10.53 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFvFFtyVTDuW"
      },
      "source": [
        "# re-arrange some files \n",
        "mv snorkel/ paht\n",
        "mv paht/snorkel .\n",
        "rm -r paht  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdEeoLUeSrhJ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNGLbm4lQIpl"
      },
      "source": [
        "## Data walk through\n",
        "\n",
        "In the files section there are two csv files: *biomimicry_function_rules.csv* and *biomimicry_functions_enumerated.csv*. \n",
        "\n",
        "biomimicry_functions_enumerated.csv\n",
        "- First column contains the labels\n",
        "- Second column contains the label id\n",
        "\n",
        "biomimicry_function_rules.csv \n",
        "- Header: these are the labels from biomimicry_functions_enumerated.csv column 1\n",
        "- Each row below the header contains words. If any word matches the text then we mark it with a label.\n",
        "\n",
        "There's another folder called `david_work` inside there's 2 csv files *formatted_enums.csv* and *labeled_data.csv*. labeled_data.csv is the important one. This contains the paper title and abstract which are combined as a text for prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziwkbwwaWStC"
      },
      "source": [
        "The code below loads the dataset and displays the columns. Not all the columns are used for prediction. Only the column called 'text' is used. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU9wsEflRN_b",
        "outputId": "5e8e278f-3fad-4d66-e214-a1ffcc805b19"
      },
      "source": [
        "from utils import load_dataset\n",
        "df_train, df_test = load_dataset() \n",
        "print(df_train.columns) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['doi', 'url', 'full_doc_link', 'is_open_access', 'label_level_1',\n",
            "       'label_level_2', 'label_level_3', 'journal', 'literature_site',\n",
            "       'unnamed: 11', 'label', 'text'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx8BVgKvWdFl"
      },
      "source": [
        "# Challenge setup\n",
        "This part of the code illustrates how to setup the environment for this challenge. This example uses snorkel. Snorkel takes a list of texts and applies rules that predict labels. S\n",
        "\n",
        "Some texts may match rules from many labels like \"attach_permanently\" and \"send_sound_signals\". This could be texts about bats. Snorkel uses supervised machine learning to take the set of matching rules and classify papers with a particular label.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpLKX0K3ae8p"
      },
      "source": [
        "## Installing prerequisites\n",
        "Run the code below to install dependancies for snorkel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JweFvhQMan6G",
        "outputId": "86cd66e1-e9d9-44b4-a189-ec270d6cf50b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "!pip install -U networkx munkres numpy scipy pandas scikit-learn"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (2.6.3)\n",
            "Collecting munkres\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 59 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.7.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.2 MB 25 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 17.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 94 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: numpy, threadpoolctl, scipy, scikit-learn, pandas, munkres\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.4 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed munkres-1.1.4 numpy-1.21.4 pandas-1.3.4 scikit-learn-1.0.1 scipy-1.7.2 threadpoolctl-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas",
                  "scipy",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfRnxdmPbJ2D",
        "outputId": "f6cef312-f427-428a-aa57-cc0f5cfa7ac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "import pandas as pd \n",
        "from snorkel.labeling import LabelingFunction\n",
        "import itertools, os\n",
        "from snorkel.labeling import PandasLFApplier\n",
        "from snorkel.labeling import LFAnalysis\n",
        "from utils import load_dataset"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c42e89590366>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msnorkel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabeling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPandasLFApplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msnorkel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabeling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLFAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_dataset' from 'utils' (snorkel/utils/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzVCvhsoborF"
      },
      "source": [
        "def keyword_lookup(x,bio_functions:pd.DataFrame,bio_function_rules:pd.DataFrame):\n",
        "    \"\"\"Snorkel Labeling function. Purpose is to return the label id based on the phrase in sentence\n",
        "\n",
        "    Args:\n",
        "        x (str): some phrase\n",
        "\n",
        "    Returns:\n",
        "        int: the id\n",
        "    \"\"\"\n",
        "    for i in range(len(bio_functions)):\n",
        "        label_name = bio_functions.iloc[i]['function'] \n",
        "        label_id = bio_functions.iloc[i]['function_enumerated']        \n",
        "        \n",
        "        label_rule_name = label_name + \"_rules\"\n",
        "        if label_rule_name in list(bio_function_rules.columns):\n",
        "            phrases_to_look_for = bio_function_rules[label_rule_name].to_list()\n",
        "            phrases_to_look_for = [x for x in phrases_to_look_for if x == 'nan']\n",
        "            for phrase in phrases_to_look_for:\n",
        "                # now you could make a counter and see the percentage match so if 10/20 phrases are in the text/abstract then you return the\n",
        "                if phrase in x.text.lower():     \n",
        "                    return label_id \n",
        "    return -1"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcPSgoT8a-o1"
      },
      "source": [
        "def create_labeling_functions(bio_file:pd.DataFrame, bio_rules:pd.DataFrame):\n",
        "    \"\"\"Takes the dataset and creates a list of labeling functions\n",
        "\n",
        "    Args:\n",
        "        bio_file (pd.DataFrame): a list of all the biomimicry functions\n",
        "        bio_rules (pd.DataFrame): a list of all the 'rules' for each biomimicry function\n",
        "\n",
        "    Returns:\n",
        "        labeling_function_list: a list of all the labeling function 'rules' corresponding to each biomimicry function\n",
        "    \"\"\"\n",
        "    bio_file = pd.read_csv(bio_file)\n",
        "    bio_rules = pd.read_csv(bio_rules)\n",
        "\n",
        "    lst = []\n",
        "    underscore_list = []\n",
        "    rules_no_na = []\n",
        "    labeling_function_list = []\n",
        "    \n",
        "    #get a list of all the rules\n",
        "    for i in range(len(bio_file)):\n",
        "        label_name = bio_file.iloc[i]['function'] \n",
        "        label_rule_name = label_name + \"_rules\"\n",
        "        if label_rule_name in list(bio_rules.columns):\n",
        "            phrases_lst = bio_rules[label_rule_name].to_list()\n",
        "            lst.append(phrases_lst)\n",
        "    chained_lst = (list(itertools.chain.from_iterable(lst)))\n",
        "    #remove blank cells\n",
        "    remove_na = [x for x in chained_lst if pd.isnull(x) == False]\n",
        "    #remove duplicates\n",
        "    for rule in remove_na:\n",
        "        if rule not in rules_no_na:\n",
        "            rules_no_na.append(rule)\n",
        "    #add underscore to rules\n",
        "    for item in rules_no_na:\n",
        "        item = item.replace(\" \", \"_\")\n",
        "        underscore_list.append(item)\n",
        "    #create labeling function for each rule\n",
        "    for phrase in underscore_list:\n",
        "        labeling_function = LabelingFunction(name=f\"keyword_{phrase}\", f=keyword_lookup,\n",
        "                        resources={\"bio_functions\":bio_file,\"bio_function_rules\":bio_rules})\n",
        "        labeling_function_list.append(labeling_function)\n",
        "\n",
        "    # print(len(labeling_function_list))\n",
        "    return labeling_function_list"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9wjHBTUbqdj",
        "outputId": "adeaa02a-707a-433f-88b9-744c67335bbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "labeling_function_list = create_labeling_functions(r'./biomimicry_functions_enumerated.csv', r'./biomimicry_function_rules.csv')\n",
        "len(labeling_function_list)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "665"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbzLdviCb4qs"
      },
      "source": [
        "## Training Problem\n",
        "The code below shows how to train using snorkel. Note the training probably won't work because it consumes an enormous about of memory.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_yq5eESb_sp",
        "outputId": "69f5af23-b489-4f2c-81de-d26cae64d1b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "from snorkel_paht import *\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    df_train, df_test = load_dataset() #import csv file and load train/test/split of dataset\n",
        "if not os.path.exists('lf_analysis.pickle'):\n",
        "    applier = PandasLFApplier(lfs=labeling_function_list)\n",
        "    # define train and test sets\n",
        "    L_train = applier.apply(df=df_train)\n",
        "    L_test = applier.apply(df=df_test)\n",
        "\n",
        "    df = LFAnalysis(L=L_train, lfs=labeling_function_list).lf_summary()\n",
        "    with open('lf_analysis.pickle','wb') as f:\n",
        "        pickle.dump({\"lf_analysis\":df, 'L_train':L_train,'L_test':L_test},f)\n",
        "\n",
        "if os.path.exists('lf_analysis.pickle'):\n",
        "    with open('lf_analysis.pickle','rb') as f:\n",
        "        data = pickle.load(f)\n",
        "        lf_analysis = data['lf_analysis']\n",
        "        L_train = data['L_train']\n",
        "        L_test = data['L_test']\n",
        "\n",
        "majority_model = MajorityLabelVoter()\n",
        "preds_train = majority_model.predict(L=L_train)\n",
        "\n",
        "label_model = LabelModel(cardinality=19, verbose=True, device='cpu')\n",
        "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=123)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-4301eaf5ac55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mapplier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPandasLFApplier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabeling_function_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# define train and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mL_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapplier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mL_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapplier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Votdntc9cbNw"
      },
      "source": [
        "### Why Training doesn't work\n",
        "Looking at L_train variable you can see that the labels go up to 97 but there's a few skips. The skipped labels are not being used but snorkel thinks that there's a total of 97 labels so it allocates the memory.  The other problem is the total number of rules. Because there's so many rules you a matrix of size #papers(trainset) x #rules where the total number of rules is 660"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxVGkOtpdM3m"
      },
      "source": [
        "# add code to show this"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYFamHt0Zo9u"
      },
      "source": [
        "# The Challenge\n",
        "The number of labels as well as the number of rules influences the size of the neural network used by snorkel. You need to find a way to programmatically reduce number of rules and labels. "
      ]
    }
  ]
}